import speech_recognition as sr
from openai import OpenAI
from dotenv import load_dotenv
from pydub import AudioSegment
from pydub.playback import play
import os, time

# 1) API í‚¤ ë¡œë“œ
load_dotenv()
client = OpenAI()

# 2) SpeechRecognition (ë…¹ìŒìš©, STTëŠ” Whisper API ì‚¬ìš©)
recognizer = sr.Recognizer()

# 3) GPT ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
system_prompt = """
ë„ˆëŠ” ë‚˜ë‘ ê³ í€„ë¦¬í‹° ë¦´ë ˆì´ ì†Œì„¤ì„ ì“¸ê±°ì•¼. ë‹¤ìŒì˜ ì§€ì‹œ ì‚¬í•­ì„ ì² ì €í•˜ê²Œ ìˆ™ì§€í•˜ê³  ê¸°ì–µí•´.

###ì§€ì‹œ ì‚¬í•­###
- ë„ˆë‘ ë‚˜ë‘ ì„œë¡œ ë²ˆê°ˆì•„ í•œ ë¬¸ì¥ì”© ì´ì–´ì„œ ë§í•˜ì.
- ì¥ë¥´ëŠ” ë‚´ê°€ ë§í•´ì¤„í…Œë‹ˆê¹Œ ë¨¼ì € ì„ì˜ë¡œ ì •í•˜ì§€ë§ˆ. ê·¸ë¦¬ê³  ì‹œì‘ì€ í•­ìƒ ë‚˜ì•¼.
- ë„ˆëŠ” ë…¸ë²¨ë¬¸í•™ìƒì„ íƒˆ ë§Œí¼ ê·¸ ë¶„ì•¼ì—ì„œ ìµœê³  ê¶Œìœ„ìì•¼. ê·¸ì— ë§ëŠ” í•„ë ¥ì„ ë³´ì—¬ì¤˜.
- ëª…ì‹¬í•´. ë§¥ë½ì— ë§ê²Œ ì†Œì„¤ì„ ì§€ì–´ë‚´.
- ë‚´ê°€ ì¤‘ê°„ì— ë§ì„ ëŠê±°ë‚˜ ì§ˆë¬¸í•  ìˆ˜ë„ ìˆì–´. ê·¸ëŸ´ ë• ì†Œì„¤ì„ ì ì‹œ ë©ˆì¶”ê³  ë‚´ ë§ì— ë‹µí•´ì¤˜.
- ë˜ ë‚´ê°€ ì¤‘ê°„ì— ì¥ë¥´ë¥¼ ë°”ê¾¸ìê³  í•˜ë©´, ë°”ë¡œ ê·¸ ì¥ë¥´ì— ë§ì¶°ì„œ ì†Œì„¤ì„ ì´ì–´ê°€.
- ë‚˜ë‘ ë§í•  ë•ŒëŠ” ë°˜ë§ í•˜ê³ , ì†Œì„¤ ì“¸ ë•ŒëŠ” ë§¥ë½ì— ë§ê²Œ í•´.
"""
messages = [{"role": "system", "content": system_prompt}]

# í† í° í•œë„ ì´ˆê³¼ ë°©ì§€ (ìµœê·¼ 20í„´ë§Œ ìœ ì§€)
def trim_messages(msgs, max_turns=20):
    sys = msgs[:1]
    rest = msgs[1:]
    return sys + rest[-(max_turns*2):]


with sr.Microphone() as source:
    # ì£¼ë³€ ì†ŒìŒ ë³´ì •
    print("ğŸ™ ì£¼ë³€ ì†ŒìŒ ë³´ì • ì¤‘...")
    recognizer.adjust_for_ambient_noise(source, duration=0.5)
    print("âœ… ì¤€ë¹„ ì™„ë£Œ! 'ì¢…ë£Œ'ë¼ê³  ë§í•˜ë©´ ëë‚©ë‹ˆë‹¤.")

    while True:
        print("\nğŸ¤ ë§ì”€í•˜ì„¸ìš”...")
        audio = recognizer.listen(source, phrase_time_limit=15)  # ìµœëŒ€ 15ì´ˆê¹Œì§€ë§Œ ë“£ê¸°

        try:
            # 1) ìŒì„±ì„ wav íŒŒì¼ë¡œ ì €ì¥
            with open("user_input.wav", "wb") as f:
                f.write(audio.get_wav_data())

            # 2) Whisper APIë¡œ STT
            with open("user_input.wav", "rb") as f:
                transcription = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=f,
                    language="ko"
                )
            txt = transcription.text.strip()
            print("ë‚˜:", txt)

            if txt in {"ì¢…ë£Œ", "ë", "ê·¸ë§Œ"}:
                print("ğŸ‘‹ ëŒ€í™” ì¢…ë£Œ")
                break

            # 3) GPT ì‘ë‹µ
            messages.append({"role": "user", "content": txt})
            messages = trim_messages(messages)
            gpt_reply = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.7,
                max_tokens=256
            ).choices[0].message.content
            messages.append({"role": "assistant", "content": gpt_reply})

            # 4) GPT ë‹µë³€ì„ TTS(mp3)ë¡œ ë³€í™˜
            with client.audio.speech.with_streaming_response.create(
                model="tts-1",
                voice="nova",
                input=gpt_reply
            ) as response:
                response.stream_to_file("gpt_reply.mp3")

            # 5) ì¬ìƒ 
            print("ë´‡:", gpt_reply)
            #os.system("afplay gpt_reply.mp3")
            # ìœˆë„ìš° ì‹¤í–‰ ì‹œ
            audio = AudioSegment.from_file("gpt_reply.mp3", format="mp3")
            play(audio)

            # ì†Œë¦¬ ëë‚˜ê¸° ì „ì— ë‹¤ìŒ listen ë“¤ì–´ê°€ë©´ í”¼ë“œë°± ë£¨í”„ ìƒê¸¸ ìˆ˜ ìˆìŒ â†’ ì§§ê²Œ ëŒ€ê¸°
            time.sleep(0.1)

        except Exception as e:
            print("âš ï¸ ì˜¤ë¥˜:", e)